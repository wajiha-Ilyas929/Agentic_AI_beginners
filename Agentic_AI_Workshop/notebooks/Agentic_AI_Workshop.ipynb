{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9859244",
   "metadata": {},
   "source": [
    "# Agentic AI Workshop ‚Äî Hands-on Jupyter Notebook\n",
    "This notebook is structured for a **2-hour interactive workshop** covering:\n",
    "\n",
    "1. Simple LLM calling\n",
    "2. Prompt engineering\n",
    "3. Tools (safe helper functions)\n",
    "4. Simple LangChain usage\n",
    "5. Types of Chains (Conversation, Summary, etc.)\n",
    "6. LangChain with Memory\n",
    "7. Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**How to use:** Run cells sequentially. If a cell fails due to missing packages, follow the error message to `pip install` the required package(s).\n",
    "\n",
    "Replace `.env` values (e.g., `OPENAI_API_KEY`) before running cells that call external APIs. There are local alternatives where possible (Chroma local vector store).\n",
    "\n",
    "-----\n",
    "Instructor notes: This notebook is \"teaching-style\" but written with clean, reusable functions so you can also show industry best practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a02a6a",
   "metadata": {},
   "source": [
    "## 0 ‚Äî Setup & Environment\n",
    "Load environment variables, check Python packages, and provide quick helper to install missing packages.\n",
    "\n",
    "Make sure you have a `.env` file in the repo root with at least:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "If you will use Anthropic or other providers, add those keys similarly to `.env`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84327e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package tiktoken not found. To install run: pip install tiktoken\n",
      "If packages are missing, run: pip install python-dotenv openai langchain chromadb tiktoken\n"
     ]
    }
   ],
   "source": [
    "# Utility: helpful installer for common packages used in this notebook.\n",
    "import sys, subprocess, pkgutil\n",
    "\n",
    "def ensure(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except Exception:\n",
    "        print(f\"Installing {package} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    finally:\n",
    "        print(f\"{package} available.\")\n",
    "\n",
    "# Common packages we will use\n",
    "packages = [\n",
    "    (\"dotenv\", \"python-dotenv\"),\n",
    "    (\"openai\", \"openai\"),\n",
    "    (\"langchain\", \"langchain\"),\n",
    "    (\"chromadb\", \"chromadb\"),\n",
    "    (\"tiktoken\", \"tiktoken\"),\n",
    "]\n",
    "\n",
    "for mod, pkg in packages:\n",
    "    try:\n",
    "        __import__(mod)\n",
    "    except Exception:\n",
    "        print(f\"Package {mod} not found. To install run: pip install {pkg}\")\n",
    "print(\"If packages are missing, run: pip install python-dotenv openai langchain chromadb tiktoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa12ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OPENAI_API_KEY found.\n",
      "Python: /home/wajihailyas/Documents/personal/Agentic_AI_Workshop/.venv/bin/python\n",
      "Working dir: /home/wajihailyas/Documents/personal/Agentic_AI_Workshop/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads .env if present\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"WARNING: OPENAI_API_KEY not found in environment. Cells using OpenAI will fail until you set it.\")\n",
    "else:\n",
    "    print(\"‚úÖ OPENAI_API_KEY found.\")\n",
    "\n",
    "# Show which Python executable is used (useful in labs)\n",
    "import sys\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Working dir:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b5ad5",
   "metadata": {},
   "source": [
    "## 1 ‚Äî Simple LLM Call\n",
    "Minimal example calling an LLM (OpenAI). The code below is written defensively so you can switch providers.\n",
    "\n",
    "This cell uses the `openai` Python client. If you prefer LangChain wrapper or Anthropic, we'll show variations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef8da065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Prompt:\n",
      " Explain Agentic AI in 3 concise bullet points. \n",
      "\n",
      "üí¨ Model Output:\n",
      "\n",
      "- **Definition**: Agentic AI refers to artificial intelligence systems that possess the ability to act autonomously, make decisions, and perform tasks without human intervention, often exhibiting goal-directed behavior.\n",
      "\n",
      "- **Capabilities**: These systems can learn from their environment, adapt to new information, and optimize their actions to achieve specific objectives, often leveraging advanced machine learning techniques.\n",
      "\n",
      "- **Applications**: Agentic AI is utilized in various fields, including robotics, autonomous vehicles, and intelligent personal assistants, where it enhances efficiency, decision-making, and operational effectiveness.\n"
     ]
    }
   ],
   "source": [
    "# Simple LLM call using OpenAI's `openai` package (modern usage with `OpenAI` client)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Please install the OpenAI Python client (pip install openai). Error: \" + str(e))\n",
    "\n",
    "import os\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError('Set OPENAI_API_KEY in .env or environment to run this cell.')\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def simple_llm_call(\n",
    "    prompt: str,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    max_tokens: int = 200,\n",
    "    temperature: float = 0.2,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Make a single LLM (chat completion) call and return the generated text.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt or instruction.\n",
    "        model (str): Model name to use (default: gpt-4o-mini).\n",
    "        max_tokens (int): Maximum number of tokens to generate.\n",
    "        temperature (float): Sampling temperature (lower = more deterministic).\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM's generated response text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error during LLM call: {e}\"\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Explain Agentic AI in 3 concise bullet points.\"\n",
    "print(\"üß† Prompt:\\n\", prompt, \"\\n\\nüí¨ Model Output:\\n\")\n",
    "print(simple_llm_call(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d5a48",
   "metadata": {},
   "source": [
    "## 2 ‚Äî Prompt Engineering Basics\n",
    "We will demonstrate how prompt structure affects outputs. We'll show:\n",
    "- A naive prompt\n",
    "- A structured prompt with system + user roles\n",
    "- A prompt with constraints and examples (few-shot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6a3706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive prompt output ===\n",
      "Agentic AI enables agents to utilize tools.\n",
      "\n",
      "=== Structured prompt output ===\n",
      "Agentic AI enables agents to utilize various tools.  \n",
      "Keywords: Agentic AI, agents, tools.\n",
      "\n",
      "=== Few-shot prompt output ===\n",
      "Summary: 'Agentic AI enables agents to utilize tools and memory for action.'  \n",
      "Keywords: agentic AI, tools, memory, action\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate prompt variations using the helper `simple_llm_call`.\n",
    "naive = \"Summarize the following text: 'Agentic AI allows agents to use tools.'\"\n",
    "structured = (\n",
    "    \"System: You are an assistant that writes concise summaries.\\n\"\n",
    "    \"User: Summarize the following text in 1 sentence and provide 3 keywords.\\n\"\n",
    "    \"Text: 'Agentic AI allows agents to use tools.'\"\n",
    ")\n",
    "few_shot = (\n",
    "    \"System: You are a helpful summarizer.\\n\"\n",
    "    \"User: Example:\\n- Text: 'RAG integrates retrieval and LLMs.'\\n- Summary: 'RAG combines retrieval with generation to ground answers.'\\n- Keywords: retrieval, grounding, LLM\\\\n\\n\"\n",
    "    \"Now summarize:\\nText: 'Agentic AI allows agents to use tools and memory to act.'\\n\"\n",
    ")\n",
    "\n",
    "print('=== Naive prompt output ===')\n",
    "print(simple_llm_call(naive))\n",
    "print('\\n=== Structured prompt output ===')\n",
    "print(simple_llm_call(structured))\n",
    "print('\\n=== Few-shot prompt output ===')\n",
    "print(simple_llm_call(few_shot))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc005e",
   "metadata": {},
   "source": [
    "## 3 ‚Äî Tools: Creating Safe Helper Functions\n",
    "Tools are functions the agent can call. We'll create several safe tools:\n",
    "- Calculator (safe eval)\n",
    "- Simple text summarizer using the LLM\n",
    "- Sentiment checker (toy)\n",
    "\n",
    "**Teaching note:** Never `eval` untrusted strings in production. Use parsers or safe expression evaluators. Here it's for demonstration in an isolated environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a61d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calc 123*7 -> 861\n",
      "summary -> Agentic AI systems use tools and information to enhance their effectiveness.\n",
      "sentiment -> positive\n"
     ]
    }
   ],
   "source": [
    "# Define some safe helper tools as pure Python functions.\n",
    "from typing import Tuple\n",
    "\n",
    "def safe_arith(expr: str) -> str:\n",
    "    \"\"\"Evaluate simple arithmetic expressions. Only allow digits and +-*/(). spaces.\n",
    "    This is a *demonstration* safe parser; it's simplistic.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    if not re.match(r\"^[0-9\\s+\\-*/().]+$\", expr):\n",
    "        return \"error: expression contains illegal characters\"\n",
    "    try:\n",
    "        # using eval with restricted builtins for demo only\n",
    "        result = eval(expr, {\"__builtins__\": None}, {})\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"error: {e}\"\n",
    "\n",
    "\n",
    "def summarizer(text: str, max_len: int = 80) -> str:\n",
    "    \"\"\"Simple LLM-based summarizer wrapper.\"\"\"\n",
    "    prompt = f\"Summarize the following text in one short sentence (<= {max_len} chars):\\n\\n{text}\"\n",
    "    return simple_llm_call(prompt, max_tokens=150)\n",
    "\n",
    "\n",
    "def sentiment_toy(text: str) -> str:\n",
    "    \"\"\"Very small toy sentiment function using keywords (not an LLM)\"\"\"\n",
    "    t = text.lower()\n",
    "    if any(w in t for w in [\"good\",\"great\",\"love\",\"excellent\",\"happy\"]):\n",
    "        return \"positive\"\n",
    "    if any(w in t for w in [\"bad\",\"terrible\",\"hate\",\"awful\",\"sad\"]):\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "# Quick demo of the tools\n",
    "print('calc 123*7 ->', safe_arith('123*7'))\n",
    "print('summary ->', summarizer('Agentic AI systems can call tools and retrieve info to act more effectively.'))\n",
    "print('sentiment ->', sentiment_toy('I love this workshop!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20eafb5",
   "metadata": {},
   "source": [
    "## 4 ‚Äî Simple LangChain (LLMChain)\n",
    "LangChain wraps LLMs with prompt templates. We'll show `LLMChain` as a building block.\n",
    "Install `langchain` and try the example below. If LangChain version mismatch occurs, pin a compatible version in `requirements.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0441ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, decision-making, and perception.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq LLM\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(\n",
    "    api_key=groq_api_key,\n",
    "    model_name=\"llama-3.3-70b-versatile\", \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Create a prompt\n",
    "template = \"\"\"\n",
    "You are an expert AI assistant.\n",
    "Answer clearly and concisely.\n",
    "\n",
    "User: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"question\"], template=template)\n",
    "\n",
    "# Create a LangChain chain\n",
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run({\"question\": \"What is artificial intelligence?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044a323",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02341925",
   "metadata": {},
   "source": [
    "## 5 ‚Äî Types of Chains in LangChain\n",
    "We'll demonstrate several chain types with small examples:\n",
    "1. LLMChain (done above)\n",
    "2. ConversationChain (simple chat memory)\n",
    "3. SequentialChain (run multiple chains in sequence)\n",
    "4. MapReduceDocumentsChain (summarization pattern)\n",
    "5. RetrievalQA (RAG preview)\n",
    "\n",
    "Note: Some chain classes may require additional packages; the notebook uses simple implementations or local fallbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1379dc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, who are you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello. I'm an artificial intelligence language model, which means I'm a computer program designed to understand and generate human-like text. I was trained on a massive dataset of text from the internet, books, and other sources, which allows me to learn patterns and relationships in language. My training data includes a wide range of topics, from science and history to entertainment and culture. I'm a bit like a super-knowledgeable librarian, but instead of just recommending books, I can have conversations and answer questions to the best of my ability. I'm constantly learning and improving, and I'm excited to chat with you. By the way, my knowledge cutoff is currently December 2023, so I may not be aware of very recent events or developments. What would you like to talk about?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, who are you?\n",
      "AI: Hello. I'm an artificial intelligence language model, which means I'm a computer program designed to understand and generate human-like text. I was trained on a massive dataset of text from the internet, books, and other sources, which allows me to learn patterns and relationships in language. My training data includes a wide range of topics, from science and history to entertainment and culture. I'm a bit like a super-knowledgeable librarian, but instead of just recommending books, I can have conversations and answer questions to the best of my ability. I'm constantly learning and improving, and I'm excited to chat with you. By the way, my knowledge cutoff is currently December 2023, so I may not be aware of very recent events or developments. What would you like to talk about?\n",
      "Human: What did I just ask you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You just asked me what I am, with the question \"Hi, who are you?\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Use the default memory_key='history'\n",
    "memory = ConversationBufferMemory(memory_key='history', return_messages=False)\n",
    "\n",
    "# Initialize the chain\n",
    "conv_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run conversation\n",
    "print(conv_chain.run(\"Hi, who are you?\"))\n",
    "print(conv_chain.run(\"What did I just ask you?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9f939b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, who are you?\n",
      "AI: Hello. I'm an artificial intelligence language model, which means I'm a computer program designed to understand and generate human-like text. I was trained on a massive dataset of text from the internet, books, and other sources, which allows me to learn patterns and relationships in language. My training data includes a wide range of topics, from science and history to entertainment and culture. I'm a bit like a super-knowledgeable librarian, but instead of just recommending books, I can have conversations and answer questions to the best of my ability. I'm constantly learning and improving, and I'm excited to chat with you. By the way, my knowledge cutoff is currently December 2023, so I may not be aware of very recent events or developments. What would you like to talk about?\n",
      "Human: What did I just ask you?\n",
      "AI: You just asked me what I am, with the question \"Hi, who are you?\"\n",
      "Human: hi\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ AI: It seems like you're greeting me again. That's perfectly fine. We've already introduced ourselves, so we can dive right into a conversation if you'd like. Is there something on your mind that you'd like to talk about, or would you like me to suggest some topics? Perhaps we could discuss a hobby, a favorite book or movie, or even explore a fascinating scientific concept? I'm all ears, or rather, all text. \n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, who are you?\n",
      "AI: Hello. I'm an artificial intelligence language model, which means I'm a computer program designed to understand and generate human-like text. I was trained on a massive dataset of text from the internet, books, and other sources, which allows me to learn patterns and relationships in language. My training data includes a wide range of topics, from science and history to entertainment and culture. I'm a bit like a super-knowledgeable librarian, but instead of just recommending books, I can have conversations and answer questions to the best of my ability. I'm constantly learning and improving, and I'm excited to chat with you. By the way, my knowledge cutoff is currently December 2023, so I may not be aware of very recent events or developments. What would you like to talk about?\n",
      "Human: What did I just ask you?\n",
      "AI: You just asked me what I am, with the question \"Hi, who are you?\"\n",
      "Human: hi\n",
      "AI: It seems like you're greeting me again. That's perfectly fine. We've already introduced ourselves, so we can dive right into a conversation if you'd like. Is there something on your mind that you'd like to talk about, or would you like me to suggest some topics? Perhaps we could discuss a hobby, a favorite book or movie, or even explore a fascinating scientific concept? I'm all ears, or rather, all text.\n",
      "Human: my name is wajiha , im AI engineer , tell me a joke about developers\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ AI: Nice to meet you, Wajiha. As an AI engineer, I'm sure you've had your fair share of laughs and frustrations when it comes to coding and development. Here's a joke for you: Why do developers prefer dark mode? Because light attracts bugs. I hope that one compiled correctly and executed a smile on your face. By the way, what kind of projects are you working on as an AI engineer? Are you more focused on natural language processing, computer vision, or something else entirely? I'm curious to know more about your work. \n",
      "\n",
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Interactive input loop\n",
    "while True:\n",
    "    user_input = input(\"üßë You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"üëã Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Generate response using conversation chain\n",
    "    response = conv_chain.run(user_input)\n",
    "    print(\"ü§ñ AI:\", response, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bd97b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99046/958600381.py:15: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(seq({'seed': 'agentic AI workshop'}))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'seed': 'agentic AI workshop', 'topic': '\"Empowering Autonomous Decision-Making: Agentic AI Workshop\"', 'description': 'The \"Empowering Autonomous Decision-Making: Agentic AI Workshop\" is an innovative forum where experts and researchers gather to explore and develop artificial intelligence systems that can make autonomous decisions, fostering a new era of agentic AI that can learn, adapt, and interact with its environment in a more human-like manner.'}\n"
     ]
    }
   ],
   "source": [
    "# SequentialChain example: run two simple chains in sequence\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Small chain1: produce a topic from a seed\n",
    "p1 = PromptTemplate(input_variables=['seed'], template='Provide a concise topic title for: {seed}')\n",
    "c1 = LLMChain(llm=llm, prompt=p1, output_key='topic')\n",
    "\n",
    "# Small chain2: expand the topic into a one-sentence description\n",
    "p2 = PromptTemplate(input_variables=['topic'], template='Write a one-sentence description for the topic: {topic}')\n",
    "c2 = LLMChain(llm=llm, prompt=p2, output_key='description')\n",
    "\n",
    "seq = SequentialChain(chains=[c1, c2], input_variables=['seed'], output_variables=['topic','description'], verbose=True)\n",
    "\n",
    "print(seq({'seed': 'agentic AI workshop'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "868b0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Split into 3 chunks\n",
      "Processing chunk 1/3...\n",
      "Processing chunk 2/3...\n",
      "Processing chunk 3/3...\n",
      "\n",
      "============================================================\n",
      "üßæ Original text length: 503 characters\n",
      "\n",
      "ü§ñ Summarized output:\n",
      "\n",
      "Here is a final concise summary that captures the main points:\n",
      "\n",
      "Agentic AI is a type of autonomous AI that can plan, decide, and use tools to achieve specific goals, differing from regular AI models. It consists of four main components: a Large Language Model, memory, tools, and a planner/controller, and is applied in areas such as personal assistants, autonomous research, and workflow automation.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# --- Step 1: Create a long Q&A-style text ---\n",
    "qa_text = \"\"\"\n",
    "Q: What is Agentic AI?\n",
    "A: Agentic AI refers to systems that can take autonomous actions to achieve goals.\n",
    "\n",
    "Q: How is it different from regular AI models?\n",
    "A: Regular AI models only respond to inputs. Agentic AI can plan, decide, and use tools to complete tasks.\n",
    "\n",
    "Q: What are the components of an agentic system?\n",
    "A: Typically, these include an LLM, memory, tools, and a planner or controller.\n",
    "\n",
    "Q: Where is Agentic AI used?\n",
    "A: In personal assistants, autonomous research systems, and workflow automation.\n",
    "\"\"\"\n",
    "\n",
    "# --- Step 2: Split the text into chunks ---\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=30)\n",
    "chunks = splitter.split_text(qa_text)\n",
    "\n",
    "print(f\"üìù Split into {len(chunks)} chunks\")\n",
    "\n",
    "# --- Step 3: Create summarization prompt ---\n",
    "summarize_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Summarize the following text concisely, capturing the main points:\n",
    "\n",
    "{text}\n",
    "\n",
    "Summary:\"\"\"\n",
    ")\n",
    "\n",
    "# --- Step 4: Method 1 - Direct summarization (for small texts) ---\n",
    "if len(chunks) <= 1:\n",
    "    # Direct summarization\n",
    "    chain = summarize_prompt | llm | StrOutputParser()\n",
    "    summary = chain.invoke({\"text\": qa_text})\n",
    "else:\n",
    "    # --- Method 2 - Map-Reduce style (for larger texts) ---\n",
    "    \n",
    "    # Step 4a: Summarize each chunk\n",
    "    chunk_summaries = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "        chain = summarize_prompt | llm | StrOutputParser()\n",
    "        chunk_summary = chain.invoke({\"text\": chunk})\n",
    "        chunk_summaries.append(chunk_summary)\n",
    "    \n",
    "    # Step 4b: Combine all chunk summaries\n",
    "    combined_summaries = \"\\n\\n\".join(chunk_summaries)\n",
    "    \n",
    "    # Step 4c: Final summary\n",
    "    final_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"The following are summaries of different parts of a document. \n",
    "Create a final concise summary that captures all the main points:\n",
    "\n",
    "{summaries}\n",
    "\n",
    "Final Summary:\"\"\"\n",
    "    )\n",
    "    chain = final_prompt | llm | StrOutputParser()\n",
    "    summary = chain.invoke({\"summaries\": combined_summaries})\n",
    "\n",
    "# --- Step 5: Display the summary ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üßæ Original text length:\", len(qa_text), \"characters\")\n",
    "print(\"\\nü§ñ Summarized output:\\n\")\n",
    "print(summary)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ebc151",
   "metadata": {},
   "source": [
    "## 7 ‚Äî RAG (Retrieval-Augmented Generation) with Chroma (local)\n",
    "We will:\n",
    "1. Create a local Chroma collection\n",
    "2. Create embeddings for small documents using LangChain/OpenAI embeddings\n",
    "3. Query the collection and show a RAG-style answer\n",
    "\n",
    "This demo is intentionally small and local so it works in lab VMs without cloud vector DBs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4ed3694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading embedding model (this may take a moment on first run)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wajihailyas/Documents/personal/Agentic_AI_Workshop/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding model loaded!\n",
      "üìö Creating vector store...\n",
      "‚úÖ Vector store created!\n",
      "\n",
      "============================================================\n",
      "üîç Query: What is RAG?\n",
      "============================================================\n",
      "\n",
      "ü§ñ Answer:\n",
      "RAG stands for Retrieval Augmented Generation. It involves retrieving relevant documents and then generating grounded answers based on them.\n",
      "\n",
      "üìÑ Source Documents:\n",
      "1. RAG stands for Retrieval Augmented Generation: you retrieve relevant docs then generate grounded answers.\n",
      "2. LangChain is a Python framework for developing applications powered by LLMs.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# RAG demo using chromadb + FREE sentence-transformers embeddings\n",
    "try:\n",
    "    import chromadb\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    from langchain.chains import RetrievalQA\n",
    "except Exception as e:\n",
    "    print('If you see import errors, install: pip install chromadb langchain-community sentence-transformers')\n",
    "    raise\n",
    "\n",
    "# Prepare documents\n",
    "docs = [\n",
    "    \"LangChain is a Python framework for developing applications powered by LLMs.\",\n",
    "    \"RAG stands for Retrieval Augmented Generation: you retrieve relevant docs then generate grounded answers.\",\n",
    "    \"Agents can use tools and memory to act autonomously.\"\n",
    "]\n",
    "\n",
    "print(\"üîÑ Loading embedding model (this may take a moment on first run)...\")\n",
    "\n",
    "# Create FREE embeddings using sentence-transformers\n",
    "# Popular models:\n",
    "# - \"all-MiniLM-L6-v2\" (lightweight, fast, 384 dimensions)\n",
    "# - \"all-mpnet-base-v2\" (better quality, 768 dimensions)\n",
    "# - \"paraphrase-multilingual-MiniLM-L12-v2\" (multilingual support)\n",
    "\n",
    "emb = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Free, fast, good quality\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'cuda' if you have GPU\n",
    "    encode_kwargs={'normalize_embeddings': True}  # Improves retrieval\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embedding model loaded!\")\n",
    "\n",
    "# Create local Chroma vector store\n",
    "print(\"üìö Creating vector store...\")\n",
    "vect = Chroma.from_texts(\n",
    "    docs, \n",
    "    embedding=emb, \n",
    "    collection_name='workshop_demo',\n",
    "    persist_directory=\"./chroma_db\"  # Optional: persist to disk\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store created!\")\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type='stuff',  # 'stuff', 'map_reduce', 'refine', or 'map_rerank'\n",
    "    retriever=vect.as_retriever(search_kwargs={\"k\": 2}),  # Retrieve top 2 docs\n",
    "    return_source_documents=True  # Optional: return source docs\n",
    ")\n",
    "\n",
    "# Query the system\n",
    "query = 'What is RAG?'\n",
    "print('\\n' + '='*60)\n",
    "print('üîç Query:', query)\n",
    "print('='*60)\n",
    "\n",
    "result = qa({\"query\": query})\n",
    "\n",
    "print('\\nü§ñ Answer:')\n",
    "print(result['result'])\n",
    "\n",
    "# Show source documents (optional)\n",
    "if 'source_documents' in result:\n",
    "    print('\\nüìÑ Source Documents:')\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"{i}. {doc.page_content}\")\n",
    "\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c054f464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Without RAG ---\n",
      "\n",
      "Retrieval Augmented Generation (RAG) is a machine learning framework that combines the strengths of information retrieval and natural language generation. It works by first retrieving relevant documents or pieces of information from a large corpus based on a given query, and then using that retrieved information to generate coherent and contextually relevant responses. This approach enhances the model's ability to produce accurate and informative outputs by grounding its responses in real-world data, thereby improving performance on tasks that require up-to-date knowledge or specific details.\n",
      "\n",
      "--- With RAG (retriever) ---\n",
      "\n",
      "Retrieval Augmented Generation (RAG) is a process where relevant documents are retrieved and then used to generate grounded answers. This approach combines the steps of retrieving relevant information and generating a response based on that information, allowing for more accurate and informed answers.\n"
     ]
    }
   ],
   "source": [
    "# Compare model answer without RAG (direct LLM) vs with RAG (retrieval-augmented)\n",
    "q = 'Explain Retrieval Augmented Generation (RAG) in one short paragraph.'\n",
    "print('--- Without RAG ---\\n')\n",
    "print(simple_llm_call(q))\n",
    "print('\\n--- With RAG (retriever) ---\\n')\n",
    "result = qa({\"query\": q})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72075d3b",
   "metadata": {},
   "source": [
    "## 8 ‚Äî Wrap-up, Exercises & Roadmap\n",
    "**Exercises for students (pick 1-2):**\n",
    "\n",
    "1. Add a new tool (e.g., a web search wrapper using SerpAPI) and integrate it into an agent.\n",
    "2. Build a multi-step SequentialChain that generates a topic, expands it, then summarizes.\n",
    "3. Improve the RAG dataset: add 10 short docs, tune `n_results`, and compare answers.\n",
    "\n",
    "**Roadmap recap:** Python ‚Üí ML fundamentals ‚Üí LLM skills ‚Üí Agent design ‚Üí Infra & safety ‚Üí portfolio projects.\n",
    "\n",
    "Good luck! You can edit this notebook to add or remove cells before the workshop. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
